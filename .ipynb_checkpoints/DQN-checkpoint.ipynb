{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mastermind import Mastermind\n",
    "from DeepQNetwork import DQNAgent\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop, Adam, Adamax, Adadelta, Adagrad, Nadam, Ftrl\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Dense\n",
    "import pygame\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1600      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                792       \n",
      "=================================================================\n",
      "Total params: 4,472\n",
      "Trainable params: 4,472\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "epoch #0\tmean reward = -350.100\tepsilon = 0.750\n",
      "epoch #1\tmean reward = 437.770\tepsilon = 0.712\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "code_to_decode = \"RBGY\"\n",
    "\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "mastermind = Mastermind(code_to_decode)\n",
    "mastermind.reset()\n",
    "\n",
    "state_size = mastermind.get_number_of_states()\n",
    "action_size = len(mastermind.get_possible_actions(None))\n",
    "states = mastermind.get_all_states()\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(64, input_dim=24))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(action_size, activation='linear'))\n",
    "\n",
    "opt = RMSprop(lr=0.0002, decay=6e-8)\n",
    "model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "agent = DQNAgent(action_size, learning_rate, model)\n",
    "\n",
    "agent.epsilon = 0.75\n",
    "\n",
    "done = False\n",
    "batch_size = 24\n",
    "EPISODES = 5\n",
    "counter = 0\n",
    "for e in range(EPISODES):\n",
    "\n",
    "    summary = []\n",
    "    for _ in range(100):\n",
    "        total_reward = 0\n",
    "        env_state = mastermind.reset()\n",
    "\n",
    "        # prepare appropriate format of the state for network\n",
    "        state = to_categorical(env_state, num_classes=state_size).reshape(1, 24)\n",
    "\n",
    "        for time in range(1000):\n",
    "            action = agent.get_action(state)\n",
    "            next_state_env, reward, done, _ = mastermind.step(states[action])\n",
    "            total_reward += reward\n",
    "\n",
    "            # prepare appropriate format of the next state for network\n",
    "            next_state = to_categorical(next_state_env, num_classes=state_size).reshape(1, 24)\n",
    "\n",
    "            # add to experience memory\n",
    "            agent.remember(state.flatten(), action, reward, next_state.flatten(), done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # train network if in the memory is more samples than size of the batch\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "        summary.append(total_reward)\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(e, np.mean(summary), agent.epsilon))\n",
    "    agent.update_epsilon_value()\n",
    "    if np.mean(summary) > 100:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mastermind = Mastermind(code_to_decode)\n",
    "done = False\n",
    "state = mastermind.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21\n",
      "-13\n",
      "24\n",
      "30\n",
      "1030\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            done = True\n",
    "\n",
    "    # prepare appropriate format of the state for network\n",
    "    state = to_categorical(state, num_classes=24).reshape(1, 24)\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, score = mastermind.step(states[action])\n",
    "    print(score)\n",
    "    clock.tick(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
